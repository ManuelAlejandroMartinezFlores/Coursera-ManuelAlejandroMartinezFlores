{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6pbN0uelIPt"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYNj6EpMlIPx",
        "outputId": "6cd22418-a02e-441a-d4bc-68450a13673a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Starting virtual X frame buffer: Xvfb.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMRrnOl_lIPz"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBefjEmLlIPz"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24XIFKsAlIP0",
        "outputId": "86fdeea0-1f25-47a7-ffcd-2130d396c8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f87a184afd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx-33IjylIP0"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz3FMJe7lIP0"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMroGwuBvgLf",
        "outputId": "fd63a036-e885-43a0-d3d5-ad90a2d0d9fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "state_dim[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1QtlTu-lIP1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz-a0Qsfw2be"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdSWIf5klIP2",
        "outputId": "edc2892b-a8d4-43d6-8ad7-f56240142220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "#<YOUR CODE: define a neural network that predicts policy logits>\n",
        "model = nn.Sequential()\n",
        "model.add_module('Dense-Input', nn.Linear(state_dim[0], 32))\n",
        "model.add_module('ReLU1', nn.ReLU())\n",
        "model.add_module('Dense', nn.Linear(32, 32))\n",
        "model.add_module('ReLU2', nn.ReLU())\n",
        "model.add_module('Actions', nn.Linear(32, n_actions))\n",
        "#model.add_module('Softmax', nn.Softmax())\n",
        "model"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (Dense-Input): Linear(in_features=4, out_features=32, bias=True)\n",
              "  (ReLU1): ReLU()\n",
              "  (Dense): Linear(in_features=32, out_features=32, bias=True)\n",
              "  (ReLU2): ReLU()\n",
              "  (Actions): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwsq7t2KlIP2"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrX8nk8llIP3"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CkQKVXXlIP5"
      },
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    #<YOUR CODE>\n",
        "    states = states.astype(np.float32)\n",
        "    states = torch.from_numpy(states)\n",
        "    logits = model(states).detach()\n",
        "    \n",
        "    return nn.functional.softmax(logits, -1).numpy()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XngIbNszlIP5"
      },
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlZrl59mlIP5"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoCt6Q9ilIP6"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "\n",
        "        # Sample action with given probabilities.\n",
        "        #a = <YOUR CODE>\n",
        "        a = np.random.choice(n_actions, p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7UbJHKZlIP6",
        "outputId": "df16cbd8-fdf4-4484-ea1a-cf4d24ee98ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsBc1TgflIP7"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBZnhb3ZlIP7"
      },
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    #<YOUR CODE>\n",
        "    rw = rewards[::-1]\n",
        "\n",
        "    for i in range(len(rewards)-1):\n",
        "        rw[i+1] += rw[i] * gamma\n",
        "\n",
        "    #return <YOUR CODE: array of cumulative rewards>\n",
        "    return rw[::-1]"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02ywIAaOlIP7",
        "outputId": "555c6ba2-dc37-4ffb-9388-507bcd78eaf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLRbNgBglIP7"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JbuN9FXlIP8"
      },
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKB4G8ymlIP9"
      },
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    J = torch.mean(log_probs_for_actions * cumulative_returns)\n",
        "    #entropy = <YOUR CODE>\n",
        "    entropy = torch.sum(probs * log_probs)\n",
        "    #loss = <YOUR CODE>\n",
        "    loss = - (J + entropy_coef * entropy)\n",
        "\n",
        "    # Gradient descent step\n",
        "    #<YOUR CODE>\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uSwwAAqlIP9"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fNVIPJ8lIP-",
        "outputId": "fa750d45-80fc-421b-ccf6-094a1f5ee2ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean reward:28.390\n",
            "mean reward:26.050\n",
            "mean reward:39.790\n",
            "mean reward:44.770\n",
            "mean reward:69.350\n",
            "mean reward:93.910\n",
            "mean reward:175.590\n",
            "mean reward:274.590\n",
            "mean reward:248.690\n",
            "mean reward:665.460\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLXHhvYhlIP-"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2GU4LnPlIP-"
      },
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG-Kp1NOlIP_"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb1PTtiDlIP_",
        "outputId": "d6aa9461-0cc4-4b4e-983b-4a1eb97c65eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'manuelalejandromartinezf@gmail.com', 'CWSbYyirUp62PI2X')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 564.92 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGO4IzH6lIP_"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}